{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient checking Affine with respect to W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.12937624 -2.84668334  3.61661502]\n",
      " [-4.3714214  -0.71148866 -0.51577405]\n",
      " [ 2.19399945  1.45658743 -0.40621823]\n",
      " [-0.78482181  2.00100755 -0.22594629]] this is my affine gradient\n",
      "[[ 1.12937624 -2.84668334  3.61661502]\n",
      " [-4.3714214  -0.71148866 -0.51577405]\n",
      " [ 2.19399945  1.45658743 -0.40621823]\n",
      " [-0.78482181  2.00100755 -0.22594629]] this is their affine gradient\n"
     ]
    }
   ],
   "source": [
    "# gradient checking: compare the analytical gradient with the numerical gradient\n",
    "# taking the affine layer as an example\n",
    "from gradient_check import eval_numerical_gradient_array\n",
    "import numpy as np\n",
    "from layers import *\n",
    "N = 5\n",
    "D = 4\n",
    "M = 3\n",
    "x = np.random.normal(size=(N, D))\n",
    "w = np.random.normal(size=(D, M))\n",
    "b = np.random.normal(size=(M, ))\n",
    "dout = np.random.normal(size=(N, M))\n",
    "\n",
    "# do a forward pass first\n",
    "out, cache = affine_forward(x, w, b)\n",
    "# check grad f/grad w, the [0] below gets the output out of the (output, cache) original output\n",
    "f=lambda w: affine_forward(x, w, b)[0]\n",
    "\n",
    "# compute the analytical gradient you wrote, [1] get the dw out of the (dx, dw, db) original output\n",
    "grad = affine_backward(dout, cache)[1]\n",
    "#print(grad)\n",
    "# compute the numerical gradient using the provided utility function\n",
    "ngrad = eval_numerical_gradient_array(f, w, dout)\n",
    "print(grad, 'this is my affine gradient')\n",
    "print(ngrad, 'this is their affine gradient')\n",
    "# they should be similar enough within some small error tolerance\n",
    "#print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine  Checking with Respect to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.0957136  -0.75567965  0.93638084 -0.16579169]\n",
      " [-2.04891708  0.38670672 -0.49181883 -4.26199382]\n",
      " [ 2.69022253  0.25111822 -0.02029018 -0.17091953]\n",
      " [-2.66378183 -0.63481211  0.82644712  0.46381368]\n",
      " [-0.75447122 -0.29284864  0.5660749  -0.32425008]] this is my affine gradient\n",
      "[[-4.0957136  -0.75567965  0.93638084 -0.16579169]\n",
      " [-2.04891708  0.38670672 -0.49181883 -4.26199382]\n",
      " [ 2.69022253  0.25111822 -0.02029018 -0.17091953]\n",
      " [-2.66378183 -0.63481211  0.82644712  0.46381368]\n",
      " [-0.75447122 -0.29284864  0.5660749  -0.32425008]] this is their affine gradient\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: affine_forward(x,w,b)[0]\n",
    "grad = affine_backward(dout,cache)[0]\n",
    "ngrad = eval_numerical_gradient_array(f, x, dout)\n",
    "print(grad, 'this is my affine gradient')\n",
    "print(ngrad, 'this is their affine gradient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine Checking with Respect to b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.04544695  4.58685785 -4.01200432] this is my affine gradient\n",
      "[-5.04544695  4.58685785 -4.01200432] this is their affine gradient\n"
     ]
    }
   ],
   "source": [
    "f = lambda b: affine_forward(x,w,b)[0]\n",
    "grad = affine_backward(dout,cache)[2]\n",
    "ngrad = eval_numerical_gradient_array(f, b, dout)\n",
    "print(grad, 'this is my affine gradient')\n",
    "print(ngrad, 'this is their affine gradient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Relu Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout_2 = np.random.uniform(size= x.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46061718 0.         0.88956867]\n",
      " [0.94622545 0.88672816 0.66799915 0.19587757]\n",
      " [0.         0.         0.3898386  0.55176336]\n",
      " [0.52736307 0.27512982 0.         0.        ]\n",
      " [0.         0.         0.926261   0.42521301]] this is my relu gradient\n",
      "[[0.         0.46061718 0.         0.88956867]\n",
      " [0.94622545 0.88672816 0.66799915 0.19587757]\n",
      " [0.         0.         0.3898386  0.55176336]\n",
      " [0.52736307 0.27512982 0.         0.        ]\n",
      " [0.         0.         0.926261   0.42521301]] this is their gradient\n"
     ]
    }
   ],
   "source": [
    "out, cache = relu_forward(x)\n",
    "grad = relu_backward(dout_2, cache)\n",
    "f = lambda x: relu_forward(x)[0]\n",
    "ngrad = eval_numerical_gradient_array(f, x, dout_2)\n",
    "\n",
    "print(grad, 'this is my relu gradient')\n",
    "print(ngrad, 'this is their gradient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Softmax Gradient, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.90382577  0.14415576  0.75967   ]\n",
      " [-0.90413718  0.05454155  0.84959563]\n",
      " [ 0.65070121 -0.67233653  0.02163531]\n",
      " [-0.91532487  0.21933155  0.69599332]\n",
      " [-0.51555631  0.22196262  0.29359369]] this is my gradient\n",
      "[[-0.90382577  0.14415576  0.75967   ]\n",
      " [-0.90413718  0.05454155  0.84959563]\n",
      " [ 0.65070121 -0.67233653  0.02163531]\n",
      " [-0.91532487  0.21933155  0.69599332]\n",
      " [-0.51555631  0.22196262  0.29359369]] this is their gradient\n"
     ]
    }
   ],
   "source": [
    "dout = 1\n",
    "x = np.random.normal(size=(N, D))\n",
    "y = np.random.randint(low=0, high=D-1, size=(N, ))\n",
    "\n",
    "loss, dx = softmax_loss(x, y)\n",
    "f = lambda x, y: softmax_loss(x, y)[0]\n",
    "grad = softmax_loss(x, y)[1]\n",
    "ngrad = eval_numerical_gradient_array(f, x, dout, y=y)\n",
    "\n",
    "print(grad, 'this is my gradient')\n",
    "print(ngrad, 'this is their gradient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example of training a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put the path to your 'hw6_mds189', which should contain a 'trainval' and 'test' directory\n",
    "path = \"/Users/Oscar Ortega/Desktop/CS189/hw6/cs189_hw6/resources/hw6_mds189\"\n",
    "from data_utils import load_mds189\n",
    "# load the dataset\n",
    "debug = False  # OPTIONAL: you can change this to True for debugging *only*. Your reported results must be with debug = False\n",
    "feat_train, label_train, feat_val, label_val = load_mds189(path,debug)\n",
    "from solver import Solver\n",
    "from classifiers.fc_net import FullyConnectedNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 3600) loss: 220.758844\n",
      "(Epoch 0 / 100) train acc: 0.148000; val_acc: 0.129167\n",
      "(Epoch 1 / 100) train acc: 0.704000; val_acc: 0.678333\n",
      "(Epoch 2 / 100) train acc: 0.689000; val_acc: 0.677500\n",
      "(Epoch 3 / 100) train acc: 0.800000; val_acc: 0.795000\n",
      "(Epoch 4 / 100) train acc: 0.651000; val_acc: 0.676667\n",
      "(Epoch 5 / 100) train acc: 0.872000; val_acc: 0.840000\n",
      "(Epoch 6 / 100) train acc: 0.906000; val_acc: 0.863333\n",
      "(Epoch 7 / 100) train acc: 0.878000; val_acc: 0.845000\n",
      "(Epoch 8 / 100) train acc: 0.904000; val_acc: 0.876667\n",
      "(Epoch 9 / 100) train acc: 0.774000; val_acc: 0.780000\n",
      "(Epoch 10 / 100) train acc: 0.571000; val_acc: 0.562500\n",
      "(Epoch 11 / 100) train acc: 0.929000; val_acc: 0.883333\n",
      "(Epoch 12 / 100) train acc: 0.935000; val_acc: 0.890833\n",
      "(Epoch 13 / 100) train acc: 0.943000; val_acc: 0.896667\n",
      "(Epoch 14 / 100) train acc: 0.953000; val_acc: 0.895833\n",
      "(Epoch 15 / 100) train acc: 0.958000; val_acc: 0.900833\n",
      "(Epoch 16 / 100) train acc: 0.747000; val_acc: 0.745000\n",
      "(Epoch 17 / 100) train acc: 0.959000; val_acc: 0.910833\n",
      "(Epoch 18 / 100) train acc: 0.964000; val_acc: 0.907500\n",
      "(Epoch 19 / 100) train acc: 0.946000; val_acc: 0.899167\n",
      "(Epoch 20 / 100) train acc: 0.962000; val_acc: 0.911667\n",
      "(Epoch 21 / 100) train acc: 0.959000; val_acc: 0.902500\n",
      "(Epoch 22 / 100) train acc: 0.967000; val_acc: 0.910833\n",
      "(Epoch 23 / 100) train acc: 0.959000; val_acc: 0.910833\n",
      "(Epoch 24 / 100) train acc: 0.959000; val_acc: 0.910833\n",
      "(Epoch 25 / 100) train acc: 0.965000; val_acc: 0.915833\n",
      "(Epoch 26 / 100) train acc: 0.965000; val_acc: 0.915000\n",
      "(Epoch 27 / 100) train acc: 0.970000; val_acc: 0.905833\n",
      "(Iteration 1001 / 3600) loss: 7.153872\n",
      "(Epoch 28 / 100) train acc: 0.964000; val_acc: 0.914167\n",
      "(Epoch 29 / 100) train acc: 0.974000; val_acc: 0.914167\n",
      "(Epoch 30 / 100) train acc: 0.966000; val_acc: 0.916667\n",
      "(Epoch 31 / 100) train acc: 0.964000; val_acc: 0.920000\n",
      "(Epoch 32 / 100) train acc: 0.978000; val_acc: 0.917500\n",
      "(Epoch 33 / 100) train acc: 0.981000; val_acc: 0.913333\n",
      "(Epoch 34 / 100) train acc: 0.972000; val_acc: 0.916667\n",
      "(Epoch 35 / 100) train acc: 0.982000; val_acc: 0.915833\n",
      "(Epoch 36 / 100) train acc: 0.982000; val_acc: 0.913333\n",
      "(Epoch 37 / 100) train acc: 0.977000; val_acc: 0.915833\n",
      "(Epoch 38 / 100) train acc: 0.969000; val_acc: 0.915000\n",
      "(Epoch 39 / 100) train acc: 0.966000; val_acc: 0.917500\n",
      "(Epoch 40 / 100) train acc: 0.983000; val_acc: 0.917500\n",
      "(Epoch 41 / 100) train acc: 0.978000; val_acc: 0.920000\n",
      "(Epoch 42 / 100) train acc: 0.959000; val_acc: 0.918333\n",
      "(Epoch 43 / 100) train acc: 0.969000; val_acc: 0.916667\n",
      "(Epoch 44 / 100) train acc: 0.976000; val_acc: 0.920833\n",
      "(Epoch 45 / 100) train acc: 0.977000; val_acc: 0.915000\n",
      "(Epoch 46 / 100) train acc: 0.972000; val_acc: 0.920833\n",
      "(Epoch 47 / 100) train acc: 0.983000; val_acc: 0.921667\n",
      "(Epoch 48 / 100) train acc: 0.987000; val_acc: 0.919167\n",
      "(Epoch 49 / 100) train acc: 0.978000; val_acc: 0.920000\n",
      "(Epoch 50 / 100) train acc: 0.983000; val_acc: 0.921667\n",
      "(Epoch 51 / 100) train acc: 0.984000; val_acc: 0.919167\n",
      "(Epoch 52 / 100) train acc: 0.977000; val_acc: 0.922500\n",
      "(Epoch 53 / 100) train acc: 0.983000; val_acc: 0.925000\n",
      "(Epoch 54 / 100) train acc: 0.982000; val_acc: 0.922500\n",
      "(Epoch 55 / 100) train acc: 0.986000; val_acc: 0.917500\n",
      "(Iteration 2001 / 3600) loss: 9.440258\n",
      "(Epoch 56 / 100) train acc: 0.986000; val_acc: 0.921667\n",
      "(Epoch 57 / 100) train acc: 0.984000; val_acc: 0.924167\n",
      "(Epoch 58 / 100) train acc: 0.989000; val_acc: 0.920833\n",
      "(Epoch 59 / 100) train acc: 0.981000; val_acc: 0.920833\n",
      "(Epoch 60 / 100) train acc: 0.984000; val_acc: 0.923333\n",
      "(Epoch 61 / 100) train acc: 0.984000; val_acc: 0.923333\n",
      "(Epoch 62 / 100) train acc: 0.982000; val_acc: 0.925000\n",
      "(Epoch 63 / 100) train acc: 0.983000; val_acc: 0.921667\n",
      "(Epoch 64 / 100) train acc: 0.984000; val_acc: 0.923333\n",
      "(Epoch 65 / 100) train acc: 0.982000; val_acc: 0.924167\n",
      "(Epoch 66 / 100) train acc: 0.989000; val_acc: 0.923333\n",
      "(Epoch 67 / 100) train acc: 0.989000; val_acc: 0.924167\n",
      "(Epoch 68 / 100) train acc: 0.988000; val_acc: 0.923333\n",
      "(Epoch 69 / 100) train acc: 0.989000; val_acc: 0.922500\n",
      "(Epoch 70 / 100) train acc: 0.989000; val_acc: 0.920833\n",
      "(Epoch 71 / 100) train acc: 0.988000; val_acc: 0.920833\n",
      "(Epoch 72 / 100) train acc: 0.989000; val_acc: 0.920833\n",
      "(Epoch 73 / 100) train acc: 0.989000; val_acc: 0.916667\n",
      "(Epoch 74 / 100) train acc: 0.990000; val_acc: 0.925833\n",
      "(Epoch 75 / 100) train acc: 0.986000; val_acc: 0.925000\n",
      "(Epoch 76 / 100) train acc: 0.991000; val_acc: 0.924167\n",
      "(Epoch 77 / 100) train acc: 0.982000; val_acc: 0.924167\n",
      "(Epoch 78 / 100) train acc: 0.979000; val_acc: 0.920833\n",
      "(Epoch 79 / 100) train acc: 0.986000; val_acc: 0.925833\n",
      "(Epoch 80 / 100) train acc: 0.989000; val_acc: 0.921667\n",
      "(Epoch 81 / 100) train acc: 0.986000; val_acc: 0.925833\n",
      "(Epoch 82 / 100) train acc: 0.987000; val_acc: 0.926667\n",
      "(Epoch 83 / 100) train acc: 0.986000; val_acc: 0.924167\n",
      "(Iteration 3001 / 3600) loss: 4.442014\n",
      "(Epoch 84 / 100) train acc: 0.990000; val_acc: 0.922500\n",
      "(Epoch 85 / 100) train acc: 0.991000; val_acc: 0.924167\n",
      "(Epoch 86 / 100) train acc: 0.987000; val_acc: 0.921667\n",
      "(Epoch 87 / 100) train acc: 0.990000; val_acc: 0.925000\n",
      "(Epoch 88 / 100) train acc: 0.990000; val_acc: 0.922500\n",
      "(Epoch 89 / 100) train acc: 0.989000; val_acc: 0.922500\n",
      "(Epoch 90 / 100) train acc: 0.991000; val_acc: 0.922500\n",
      "(Epoch 91 / 100) train acc: 0.994000; val_acc: 0.924167\n",
      "(Epoch 92 / 100) train acc: 0.989000; val_acc: 0.925833\n",
      "(Epoch 93 / 100) train acc: 0.994000; val_acc: 0.922500\n",
      "(Epoch 94 / 100) train acc: 0.985000; val_acc: 0.924167\n",
      "(Epoch 95 / 100) train acc: 0.990000; val_acc: 0.925000\n",
      "(Epoch 96 / 100) train acc: 0.988000; val_acc: 0.925833\n",
      "(Epoch 97 / 100) train acc: 0.991000; val_acc: 0.925833\n",
      "(Epoch 98 / 100) train acc: 0.990000; val_acc: 0.925000\n",
      "(Epoch 99 / 100) train acc: 0.991000; val_acc: 0.925833\n",
      "(Epoch 100 / 100) train acc: 0.990000; val_acc: 0.923333\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "      'X_train': feat_train,\n",
    "      'y_train': label_train,\n",
    "      'X_val': feat_val,\n",
    "      'y_val': label_val}\n",
    "\n",
    "# TODO: fill out the hyperparamets\n",
    "hyperparams = {'lr_decay': .99,\n",
    "               'num_epochs': 100,\n",
    "               'batch_size': 100,\n",
    "               'learning_rate': 1e-5,\n",
    "               'weight_scale': .01\n",
    "              }\n",
    "\n",
    "# TODO: fill out the number of units in your hidden layers\n",
    "hidden_dim = [150, 150] # this should be a list of units for each hiddent layer\n",
    "\n",
    "model = FullyConnectedNet(input_dim=75,\n",
    "                          hidden_dim=hidden_dim, \n",
    "                          weight_scale = hyperparams['weight_scale'])\n",
    "solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': hyperparams['learning_rate'],\n",
    "                },\n",
    "                lr_decay=hyperparams['lr_decay'],\n",
    "                num_epochs=hyperparams['num_epochs'], \n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 7200) loss: 1030.872534\n",
      "(Epoch 0 / 200) train acc: 0.194000; val_acc: 0.198333\n",
      "(Epoch 1 / 200) train acc: 0.275000; val_acc: 0.291667\n",
      "(Epoch 2 / 200) train acc: 0.421000; val_acc: 0.426667\n",
      "(Epoch 3 / 200) train acc: 0.480000; val_acc: 0.453333\n",
      "(Epoch 4 / 200) train acc: 0.537000; val_acc: 0.513333\n",
      "(Epoch 5 / 200) train acc: 0.547000; val_acc: 0.520833\n",
      "(Epoch 6 / 200) train acc: 0.543000; val_acc: 0.547500\n",
      "(Epoch 7 / 200) train acc: 0.599000; val_acc: 0.575833\n",
      "(Epoch 8 / 200) train acc: 0.589000; val_acc: 0.591667\n",
      "(Epoch 9 / 200) train acc: 0.632000; val_acc: 0.607500\n",
      "(Epoch 10 / 200) train acc: 0.619000; val_acc: 0.629167\n",
      "(Epoch 11 / 200) train acc: 0.635000; val_acc: 0.616667\n",
      "(Epoch 12 / 200) train acc: 0.650000; val_acc: 0.627500\n",
      "(Epoch 13 / 200) train acc: 0.654000; val_acc: 0.625833\n",
      "(Epoch 14 / 200) train acc: 0.681000; val_acc: 0.665000\n",
      "(Epoch 15 / 200) train acc: 0.701000; val_acc: 0.651667\n",
      "(Epoch 16 / 200) train acc: 0.670000; val_acc: 0.675000\n",
      "(Epoch 17 / 200) train acc: 0.706000; val_acc: 0.695000\n",
      "(Epoch 18 / 200) train acc: 0.711000; val_acc: 0.696667\n",
      "(Epoch 19 / 200) train acc: 0.705000; val_acc: 0.680000\n",
      "(Epoch 20 / 200) train acc: 0.756000; val_acc: 0.710000\n",
      "(Epoch 21 / 200) train acc: 0.740000; val_acc: 0.721667\n",
      "(Epoch 22 / 200) train acc: 0.744000; val_acc: 0.717500\n",
      "(Epoch 23 / 200) train acc: 0.754000; val_acc: 0.725000\n",
      "(Epoch 24 / 200) train acc: 0.776000; val_acc: 0.745000\n",
      "(Epoch 25 / 200) train acc: 0.761000; val_acc: 0.734167\n",
      "(Epoch 26 / 200) train acc: 0.784000; val_acc: 0.757500\n",
      "(Epoch 27 / 200) train acc: 0.794000; val_acc: 0.740833\n",
      "(Iteration 1001 / 7200) loss: 71.267290\n",
      "(Epoch 28 / 200) train acc: 0.734000; val_acc: 0.686667\n",
      "(Epoch 29 / 200) train acc: 0.815000; val_acc: 0.775833\n",
      "(Epoch 30 / 200) train acc: 0.803000; val_acc: 0.770833\n",
      "(Epoch 31 / 200) train acc: 0.851000; val_acc: 0.770000\n",
      "(Epoch 32 / 200) train acc: 0.859000; val_acc: 0.785000\n",
      "(Epoch 33 / 200) train acc: 0.806000; val_acc: 0.751667\n",
      "(Epoch 34 / 200) train acc: 0.841000; val_acc: 0.783333\n",
      "(Epoch 35 / 200) train acc: 0.838000; val_acc: 0.789167\n",
      "(Epoch 36 / 200) train acc: 0.810000; val_acc: 0.785833\n",
      "(Epoch 37 / 200) train acc: 0.844000; val_acc: 0.800000\n",
      "(Epoch 38 / 200) train acc: 0.862000; val_acc: 0.807500\n",
      "(Epoch 39 / 200) train acc: 0.861000; val_acc: 0.810833\n",
      "(Epoch 40 / 200) train acc: 0.830000; val_acc: 0.802500\n",
      "(Epoch 41 / 200) train acc: 0.882000; val_acc: 0.812500\n",
      "(Epoch 42 / 200) train acc: 0.885000; val_acc: 0.820000\n",
      "(Epoch 43 / 200) train acc: 0.868000; val_acc: 0.797500\n",
      "(Epoch 44 / 200) train acc: 0.867000; val_acc: 0.819167\n",
      "(Epoch 45 / 200) train acc: 0.880000; val_acc: 0.824167\n",
      "(Epoch 46 / 200) train acc: 0.892000; val_acc: 0.830833\n",
      "(Epoch 47 / 200) train acc: 0.885000; val_acc: 0.825833\n",
      "(Epoch 48 / 200) train acc: 0.874000; val_acc: 0.821667\n",
      "(Epoch 49 / 200) train acc: 0.875000; val_acc: 0.823333\n",
      "(Epoch 50 / 200) train acc: 0.876000; val_acc: 0.832500\n",
      "(Epoch 51 / 200) train acc: 0.900000; val_acc: 0.837500\n",
      "(Epoch 52 / 200) train acc: 0.902000; val_acc: 0.845000\n",
      "(Epoch 53 / 200) train acc: 0.900000; val_acc: 0.841667\n",
      "(Epoch 54 / 200) train acc: 0.902000; val_acc: 0.843333\n",
      "(Epoch 55 / 200) train acc: 0.912000; val_acc: 0.834167\n",
      "(Iteration 2001 / 7200) loss: 23.211329\n",
      "(Epoch 56 / 200) train acc: 0.912000; val_acc: 0.848333\n",
      "(Epoch 57 / 200) train acc: 0.911000; val_acc: 0.846667\n",
      "(Epoch 58 / 200) train acc: 0.905000; val_acc: 0.848333\n",
      "(Epoch 59 / 200) train acc: 0.898000; val_acc: 0.848333\n",
      "(Epoch 60 / 200) train acc: 0.910000; val_acc: 0.855000\n",
      "(Epoch 61 / 200) train acc: 0.923000; val_acc: 0.853333\n",
      "(Epoch 62 / 200) train acc: 0.903000; val_acc: 0.852500\n",
      "(Epoch 63 / 200) train acc: 0.924000; val_acc: 0.858333\n",
      "(Epoch 64 / 200) train acc: 0.909000; val_acc: 0.852500\n",
      "(Epoch 65 / 200) train acc: 0.927000; val_acc: 0.861667\n",
      "(Epoch 66 / 200) train acc: 0.904000; val_acc: 0.855000\n",
      "(Epoch 67 / 200) train acc: 0.918000; val_acc: 0.860000\n",
      "(Epoch 68 / 200) train acc: 0.923000; val_acc: 0.857500\n",
      "(Epoch 69 / 200) train acc: 0.931000; val_acc: 0.860000\n",
      "(Epoch 70 / 200) train acc: 0.919000; val_acc: 0.860000\n",
      "(Epoch 71 / 200) train acc: 0.931000; val_acc: 0.865000\n",
      "(Epoch 72 / 200) train acc: 0.939000; val_acc: 0.862500\n",
      "(Epoch 73 / 200) train acc: 0.945000; val_acc: 0.865000\n",
      "(Epoch 74 / 200) train acc: 0.944000; val_acc: 0.865000\n",
      "(Epoch 75 / 200) train acc: 0.920000; val_acc: 0.864167\n",
      "(Epoch 76 / 200) train acc: 0.930000; val_acc: 0.859167\n",
      "(Epoch 77 / 200) train acc: 0.947000; val_acc: 0.865833\n",
      "(Epoch 78 / 200) train acc: 0.930000; val_acc: 0.865000\n",
      "(Epoch 79 / 200) train acc: 0.941000; val_acc: 0.867500\n",
      "(Epoch 80 / 200) train acc: 0.939000; val_acc: 0.864167\n",
      "(Epoch 81 / 200) train acc: 0.926000; val_acc: 0.870000\n",
      "(Epoch 82 / 200) train acc: 0.936000; val_acc: 0.871667\n",
      "(Epoch 83 / 200) train acc: 0.956000; val_acc: 0.867500\n",
      "(Iteration 3001 / 7200) loss: 21.368829\n",
      "(Epoch 84 / 200) train acc: 0.939000; val_acc: 0.865000\n",
      "(Epoch 85 / 200) train acc: 0.934000; val_acc: 0.868333\n",
      "(Epoch 86 / 200) train acc: 0.937000; val_acc: 0.871667\n",
      "(Epoch 87 / 200) train acc: 0.951000; val_acc: 0.874167\n",
      "(Epoch 88 / 200) train acc: 0.929000; val_acc: 0.875833\n",
      "(Epoch 89 / 200) train acc: 0.946000; val_acc: 0.871667\n",
      "(Epoch 90 / 200) train acc: 0.935000; val_acc: 0.866667\n",
      "(Epoch 91 / 200) train acc: 0.947000; val_acc: 0.874167\n",
      "(Epoch 92 / 200) train acc: 0.947000; val_acc: 0.873333\n",
      "(Epoch 93 / 200) train acc: 0.946000; val_acc: 0.873333\n",
      "(Epoch 94 / 200) train acc: 0.949000; val_acc: 0.873333\n",
      "(Epoch 95 / 200) train acc: 0.946000; val_acc: 0.873333\n",
      "(Epoch 96 / 200) train acc: 0.951000; val_acc: 0.873333\n",
      "(Epoch 97 / 200) train acc: 0.953000; val_acc: 0.871667\n",
      "(Epoch 98 / 200) train acc: 0.948000; val_acc: 0.875833\n",
      "(Epoch 99 / 200) train acc: 0.958000; val_acc: 0.873333\n",
      "(Epoch 100 / 200) train acc: 0.950000; val_acc: 0.876667\n",
      "(Epoch 101 / 200) train acc: 0.961000; val_acc: 0.874167\n",
      "(Epoch 102 / 200) train acc: 0.967000; val_acc: 0.874167\n",
      "(Epoch 103 / 200) train acc: 0.947000; val_acc: 0.869167\n",
      "(Epoch 104 / 200) train acc: 0.932000; val_acc: 0.874167\n",
      "(Epoch 105 / 200) train acc: 0.953000; val_acc: 0.876667\n",
      "(Epoch 106 / 200) train acc: 0.954000; val_acc: 0.876667\n",
      "(Epoch 107 / 200) train acc: 0.947000; val_acc: 0.873333\n",
      "(Epoch 108 / 200) train acc: 0.946000; val_acc: 0.873333\n",
      "(Epoch 109 / 200) train acc: 0.950000; val_acc: 0.876667\n",
      "(Epoch 110 / 200) train acc: 0.947000; val_acc: 0.875000\n",
      "(Epoch 111 / 200) train acc: 0.954000; val_acc: 0.875000\n",
      "(Iteration 4001 / 7200) loss: 16.134351\n",
      "(Epoch 112 / 200) train acc: 0.965000; val_acc: 0.874167\n",
      "(Epoch 113 / 200) train acc: 0.958000; val_acc: 0.877500\n",
      "(Epoch 114 / 200) train acc: 0.937000; val_acc: 0.876667\n",
      "(Epoch 115 / 200) train acc: 0.968000; val_acc: 0.876667\n",
      "(Epoch 116 / 200) train acc: 0.936000; val_acc: 0.876667\n",
      "(Epoch 117 / 200) train acc: 0.952000; val_acc: 0.875833\n",
      "(Epoch 118 / 200) train acc: 0.956000; val_acc: 0.876667\n",
      "(Epoch 119 / 200) train acc: 0.959000; val_acc: 0.872500\n",
      "(Epoch 120 / 200) train acc: 0.948000; val_acc: 0.875833\n",
      "(Epoch 121 / 200) train acc: 0.957000; val_acc: 0.876667\n",
      "(Epoch 122 / 200) train acc: 0.956000; val_acc: 0.876667\n",
      "(Epoch 123 / 200) train acc: 0.956000; val_acc: 0.878333\n",
      "(Epoch 124 / 200) train acc: 0.950000; val_acc: 0.877500\n",
      "(Epoch 125 / 200) train acc: 0.956000; val_acc: 0.876667\n",
      "(Epoch 126 / 200) train acc: 0.964000; val_acc: 0.875833\n",
      "(Epoch 127 / 200) train acc: 0.946000; val_acc: 0.875000\n",
      "(Epoch 128 / 200) train acc: 0.958000; val_acc: 0.877500\n",
      "(Epoch 129 / 200) train acc: 0.945000; val_acc: 0.876667\n",
      "(Epoch 130 / 200) train acc: 0.951000; val_acc: 0.877500\n",
      "(Epoch 131 / 200) train acc: 0.962000; val_acc: 0.879167\n",
      "(Epoch 132 / 200) train acc: 0.959000; val_acc: 0.876667\n",
      "(Epoch 133 / 200) train acc: 0.956000; val_acc: 0.877500\n",
      "(Epoch 134 / 200) train acc: 0.955000; val_acc: 0.877500\n",
      "(Epoch 135 / 200) train acc: 0.950000; val_acc: 0.875833\n",
      "(Epoch 136 / 200) train acc: 0.960000; val_acc: 0.880000\n",
      "(Epoch 137 / 200) train acc: 0.957000; val_acc: 0.879167\n",
      "(Epoch 138 / 200) train acc: 0.958000; val_acc: 0.879167\n",
      "(Iteration 5001 / 7200) loss: 30.188754\n",
      "(Epoch 139 / 200) train acc: 0.950000; val_acc: 0.879167\n",
      "(Epoch 140 / 200) train acc: 0.957000; val_acc: 0.878333\n",
      "(Epoch 141 / 200) train acc: 0.963000; val_acc: 0.877500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 142 / 200) train acc: 0.959000; val_acc: 0.880833\n",
      "(Epoch 143 / 200) train acc: 0.960000; val_acc: 0.881667\n",
      "(Epoch 144 / 200) train acc: 0.968000; val_acc: 0.881667\n",
      "(Epoch 145 / 200) train acc: 0.960000; val_acc: 0.882500\n",
      "(Epoch 146 / 200) train acc: 0.960000; val_acc: 0.883333\n",
      "(Epoch 147 / 200) train acc: 0.966000; val_acc: 0.882500\n",
      "(Epoch 148 / 200) train acc: 0.960000; val_acc: 0.880833\n",
      "(Epoch 149 / 200) train acc: 0.945000; val_acc: 0.875833\n",
      "(Epoch 150 / 200) train acc: 0.969000; val_acc: 0.882500\n",
      "(Epoch 151 / 200) train acc: 0.965000; val_acc: 0.883333\n",
      "(Epoch 152 / 200) train acc: 0.966000; val_acc: 0.881667\n",
      "(Epoch 153 / 200) train acc: 0.949000; val_acc: 0.882500\n",
      "(Epoch 154 / 200) train acc: 0.964000; val_acc: 0.883333\n",
      "(Epoch 155 / 200) train acc: 0.966000; val_acc: 0.882500\n",
      "(Epoch 156 / 200) train acc: 0.950000; val_acc: 0.884167\n",
      "(Epoch 157 / 200) train acc: 0.958000; val_acc: 0.883333\n",
      "(Epoch 158 / 200) train acc: 0.964000; val_acc: 0.883333\n",
      "(Epoch 159 / 200) train acc: 0.964000; val_acc: 0.884167\n",
      "(Epoch 160 / 200) train acc: 0.970000; val_acc: 0.884167\n",
      "(Epoch 161 / 200) train acc: 0.967000; val_acc: 0.881667\n",
      "(Epoch 162 / 200) train acc: 0.954000; val_acc: 0.885000\n",
      "(Epoch 163 / 200) train acc: 0.961000; val_acc: 0.882500\n",
      "(Epoch 164 / 200) train acc: 0.960000; val_acc: 0.879167\n",
      "(Epoch 165 / 200) train acc: 0.969000; val_acc: 0.884167\n",
      "(Epoch 166 / 200) train acc: 0.970000; val_acc: 0.885000\n",
      "(Iteration 6001 / 7200) loss: 12.029299\n",
      "(Epoch 167 / 200) train acc: 0.953000; val_acc: 0.885000\n",
      "(Epoch 168 / 200) train acc: 0.960000; val_acc: 0.884167\n",
      "(Epoch 169 / 200) train acc: 0.954000; val_acc: 0.885000\n",
      "(Epoch 170 / 200) train acc: 0.969000; val_acc: 0.882500\n",
      "(Epoch 171 / 200) train acc: 0.968000; val_acc: 0.884167\n",
      "(Epoch 172 / 200) train acc: 0.965000; val_acc: 0.886667\n",
      "(Epoch 173 / 200) train acc: 0.966000; val_acc: 0.885000\n",
      "(Epoch 174 / 200) train acc: 0.962000; val_acc: 0.884167\n",
      "(Epoch 175 / 200) train acc: 0.954000; val_acc: 0.882500\n",
      "(Epoch 176 / 200) train acc: 0.970000; val_acc: 0.883333\n",
      "(Epoch 177 / 200) train acc: 0.960000; val_acc: 0.884167\n",
      "(Epoch 178 / 200) train acc: 0.958000; val_acc: 0.885000\n",
      "(Epoch 179 / 200) train acc: 0.977000; val_acc: 0.885000\n",
      "(Epoch 180 / 200) train acc: 0.965000; val_acc: 0.886667\n",
      "(Epoch 181 / 200) train acc: 0.962000; val_acc: 0.886667\n",
      "(Epoch 182 / 200) train acc: 0.969000; val_acc: 0.886667\n",
      "(Epoch 183 / 200) train acc: 0.963000; val_acc: 0.887500\n",
      "(Epoch 184 / 200) train acc: 0.965000; val_acc: 0.888333\n",
      "(Epoch 185 / 200) train acc: 0.966000; val_acc: 0.885833\n",
      "(Epoch 186 / 200) train acc: 0.959000; val_acc: 0.885833\n",
      "(Epoch 187 / 200) train acc: 0.963000; val_acc: 0.886667\n",
      "(Epoch 188 / 200) train acc: 0.948000; val_acc: 0.885833\n",
      "(Epoch 189 / 200) train acc: 0.966000; val_acc: 0.888333\n",
      "(Epoch 190 / 200) train acc: 0.972000; val_acc: 0.889167\n",
      "(Epoch 191 / 200) train acc: 0.963000; val_acc: 0.888333\n",
      "(Epoch 192 / 200) train acc: 0.965000; val_acc: 0.885833\n",
      "(Epoch 193 / 200) train acc: 0.957000; val_acc: 0.887500\n",
      "(Epoch 194 / 200) train acc: 0.954000; val_acc: 0.887500\n",
      "(Iteration 7001 / 7200) loss: 16.633236\n",
      "(Epoch 195 / 200) train acc: 0.969000; val_acc: 0.887500\n",
      "(Epoch 196 / 200) train acc: 0.966000; val_acc: 0.886667\n",
      "(Epoch 197 / 200) train acc: 0.961000; val_acc: 0.889167\n",
      "(Epoch 198 / 200) train acc: 0.976000; val_acc: 0.888333\n",
      "(Epoch 199 / 200) train acc: 0.965000; val_acc: 0.888333\n",
      "(Epoch 200 / 200) train acc: 0.966000; val_acc: 0.888333\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "      'X_train': feat_train,\n",
    "      'y_train': label_train,\n",
    "      'X_val': feat_val,\n",
    "      'y_val': label_val}\n",
    "\n",
    "# TODO: fill out the hyperparamets\n",
    "hyperparams = {'lr_decay': 0.99,\n",
    "               'num_epochs': 200,\n",
    "               'batch_size': 100,\n",
    "               'learning_rate': 1e-5\n",
    "              }\n",
    "\n",
    "# TODO: fill out the number of units in your hidden layers\n",
    "hidden_dim = [30, 20, 20] # this should be a list of units for each hiddent layer\n",
    "\n",
    "model = FullyConnectedNet(input_dim=75,\n",
    "                          hidden_dim=hidden_dim)\n",
    "solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': hyperparams['learning_rate'],\n",
    "                },\n",
    "                lr_decay=hyperparams['lr_decay'],\n",
    "                num_epochs=hyperparams['num_epochs'], \n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
