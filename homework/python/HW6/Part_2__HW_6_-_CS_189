{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example of gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8681333  -0.30342522]\n",
      " [ 0.3425241  -0.34390659]] this is my affine gradient\n",
      "[[-0.8681333  -0.30342522]\n",
      " [ 0.3425241  -0.34390659]] this is their affine gradient\n"
     ]
    }
   ],
   "source": [
    "# gradient checking: compare the analytical gradient with the numerical gradient\n",
    "# taking the affine layer as an example\n",
    "from gradient_check import eval_numerical_gradient_array\n",
    "import numpy as np\n",
    "from layers import *\n",
    "N = 2\n",
    "D = 2\n",
    "M = 2\n",
    "x = np.random.normal(size=(N, D))\n",
    "w = np.random.normal(size=(D, M))\n",
    "b = np.random.normal(size=(M, ))\n",
    "dout = np.random.normal(size=(N, M))\n",
    "\n",
    "# do a forward pass first\n",
    "out, cache = affine_forward(x, w, b)\n",
    "# check grad f/grad w, the [0] below gets the output out of the (output, cache) original output\n",
    "f=lambda w: affine_forward(x, w, b)[0]\n",
    "# compute the analytical gradient you wrote, [1] get the dw out of the (dx, dw, db) original output\n",
    "grad = affine_backward(dout, cache)[1]\n",
    "# compute the numerical gradient using the provided utility function\n",
    "ngrad = eval_numerical_gradient_array(f, w, dout)\n",
    "print(grad, 'this is my affine gradient')\n",
    "print(ngrad, 'this is their affine gradient')\n",
    "# they should be similar enough within some small error tolerance\n",
    "#print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Relu Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout_2 = np.random.uniform(size= x.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0761284  0.        ]\n",
      " [0.60174612 0.74240341]] this is my relu gradient\n",
      "[[0.0761284  0.        ]\n",
      " [0.60174612 0.74240341]] this is their gradient\n"
     ]
    }
   ],
   "source": [
    "out, cache = relu_forward(x)\n",
    "grad = relu_backward(dout_2, cache)\n",
    "f = lambda x: relu_forward(x)[0]\n",
    "ngrad = eval_numerical_gradient_array(f, x, dout_2)\n",
    "\n",
    "print(grad, 'this is my relu gradient')\n",
    "print(ngrad, 'this is their gradient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Softmax Gradient, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.47117201  0.47117201]\n",
      " [-0.69279048  0.69279048]]\n",
      "[[-0.47117201  0.47117201]\n",
      " [-0.69279048  0.69279048]]\n"
     ]
    }
   ],
   "source": [
    "dout = 1\n",
    "x = np.random.normal(size=(N, D))\n",
    "y = np.random.randint(low=0, high=D-1, size=(N, ))\n",
    "\n",
    "loss, dx = softmax_loss(x, y)\n",
    "f = lambda x, y: softmax_loss(x, y)[0]\n",
    "grad = softmax_loss(x, y)[1]\n",
    "ngrad = eval_numerical_gradient_array(f, x, dout, y=y)\n",
    "\n",
    "print(grad)\n",
    "print(ngrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example of training a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put the path to your 'hw6_mds189', which should contain a 'trainval' and 'test' directory\n",
    "path = \"/Users/Oscar Ortega/Desktop/CS189/hw6/cs189_hw6/resources/hw6_mds189\"\n",
    "from data_utils import load_mds189\n",
    "# load the dataset\n",
    "debug = False  # OPTIONAL: you can change this to True for debugging *only*. Your reported results must be with debug = False\n",
    "feat_train, label_train, feat_val, label_val = load_mds189(path,debug)\n",
    "from solver import Solver\n",
    "from classifiers.fc_net import FullyConnectedNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 3600) loss: 225.364143\n",
      "(Epoch 0 / 100) train acc: 0.179000; val_acc: 0.181667\n",
      "(Epoch 1 / 100) train acc: 0.505000; val_acc: 0.537500\n",
      "(Epoch 2 / 100) train acc: 0.691000; val_acc: 0.663333\n",
      "(Epoch 3 / 100) train acc: 0.657000; val_acc: 0.647500\n",
      "(Epoch 4 / 100) train acc: 0.627000; val_acc: 0.600833\n",
      "(Epoch 5 / 100) train acc: 0.853000; val_acc: 0.845000\n",
      "(Epoch 6 / 100) train acc: 0.720000; val_acc: 0.724167\n",
      "(Epoch 7 / 100) train acc: 0.640000; val_acc: 0.652500\n",
      "(Epoch 8 / 100) train acc: 0.910000; val_acc: 0.879167\n",
      "(Epoch 9 / 100) train acc: 0.916000; val_acc: 0.866667\n",
      "(Epoch 10 / 100) train acc: 0.830000; val_acc: 0.805000\n",
      "(Epoch 11 / 100) train acc: 0.871000; val_acc: 0.835833\n",
      "(Epoch 12 / 100) train acc: 0.930000; val_acc: 0.898333\n",
      "(Epoch 13 / 100) train acc: 0.904000; val_acc: 0.868333\n",
      "(Epoch 14 / 100) train acc: 0.942000; val_acc: 0.905000\n",
      "(Epoch 15 / 100) train acc: 0.919000; val_acc: 0.895000\n",
      "(Epoch 16 / 100) train acc: 0.941000; val_acc: 0.898333\n",
      "(Epoch 17 / 100) train acc: 0.940000; val_acc: 0.899167\n",
      "(Epoch 18 / 100) train acc: 0.941000; val_acc: 0.898333\n",
      "(Epoch 19 / 100) train acc: 0.969000; val_acc: 0.903333\n",
      "(Epoch 20 / 100) train acc: 0.941000; val_acc: 0.903333\n",
      "(Epoch 21 / 100) train acc: 0.952000; val_acc: 0.907500\n",
      "(Epoch 22 / 100) train acc: 0.947000; val_acc: 0.904167\n",
      "(Epoch 23 / 100) train acc: 0.958000; val_acc: 0.915000\n",
      "(Epoch 24 / 100) train acc: 0.959000; val_acc: 0.908333\n",
      "(Epoch 25 / 100) train acc: 0.961000; val_acc: 0.910000\n",
      "(Epoch 26 / 100) train acc: 0.962000; val_acc: 0.915833\n",
      "(Epoch 27 / 100) train acc: 0.967000; val_acc: 0.915000\n",
      "(Iteration 1001 / 3600) loss: 13.387611\n",
      "(Epoch 28 / 100) train acc: 0.959000; val_acc: 0.915000\n",
      "(Epoch 29 / 100) train acc: 0.973000; val_acc: 0.910833\n",
      "(Epoch 30 / 100) train acc: 0.968000; val_acc: 0.915833\n",
      "(Epoch 31 / 100) train acc: 0.976000; val_acc: 0.915833\n",
      "(Epoch 32 / 100) train acc: 0.973000; val_acc: 0.916667\n",
      "(Epoch 33 / 100) train acc: 0.969000; val_acc: 0.915833\n",
      "(Epoch 34 / 100) train acc: 0.976000; val_acc: 0.912500\n",
      "(Epoch 35 / 100) train acc: 0.981000; val_acc: 0.915000\n",
      "(Epoch 36 / 100) train acc: 0.971000; val_acc: 0.910833\n",
      "(Epoch 37 / 100) train acc: 0.972000; val_acc: 0.916667\n",
      "(Epoch 38 / 100) train acc: 0.969000; val_acc: 0.917500\n",
      "(Epoch 39 / 100) train acc: 0.973000; val_acc: 0.915833\n",
      "(Epoch 40 / 100) train acc: 0.980000; val_acc: 0.914167\n",
      "(Epoch 41 / 100) train acc: 0.976000; val_acc: 0.915000\n",
      "(Epoch 42 / 100) train acc: 0.971000; val_acc: 0.915833\n",
      "(Epoch 43 / 100) train acc: 0.962000; val_acc: 0.917500\n",
      "(Epoch 44 / 100) train acc: 0.978000; val_acc: 0.915833\n",
      "(Epoch 45 / 100) train acc: 0.971000; val_acc: 0.913333\n",
      "(Epoch 46 / 100) train acc: 0.977000; val_acc: 0.915833\n",
      "(Epoch 47 / 100) train acc: 0.982000; val_acc: 0.916667\n",
      "(Epoch 48 / 100) train acc: 0.984000; val_acc: 0.916667\n",
      "(Epoch 49 / 100) train acc: 0.978000; val_acc: 0.913333\n",
      "(Epoch 50 / 100) train acc: 0.977000; val_acc: 0.917500\n",
      "(Epoch 51 / 100) train acc: 0.982000; val_acc: 0.914167\n",
      "(Epoch 52 / 100) train acc: 0.982000; val_acc: 0.918333\n",
      "(Epoch 53 / 100) train acc: 0.981000; val_acc: 0.915833\n",
      "(Epoch 54 / 100) train acc: 0.982000; val_acc: 0.920000\n",
      "(Epoch 55 / 100) train acc: 0.974000; val_acc: 0.918333\n",
      "(Iteration 2001 / 3600) loss: 7.953663\n",
      "(Epoch 56 / 100) train acc: 0.985000; val_acc: 0.918333\n",
      "(Epoch 57 / 100) train acc: 0.985000; val_acc: 0.915833\n",
      "(Epoch 58 / 100) train acc: 0.986000; val_acc: 0.920000\n",
      "(Epoch 59 / 100) train acc: 0.984000; val_acc: 0.921667\n",
      "(Epoch 60 / 100) train acc: 0.982000; val_acc: 0.921667\n",
      "(Epoch 61 / 100) train acc: 0.985000; val_acc: 0.918333\n",
      "(Epoch 62 / 100) train acc: 0.988000; val_acc: 0.917500\n",
      "(Epoch 63 / 100) train acc: 0.989000; val_acc: 0.921667\n",
      "(Epoch 64 / 100) train acc: 0.984000; val_acc: 0.924167\n",
      "(Epoch 65 / 100) train acc: 0.980000; val_acc: 0.916667\n",
      "(Epoch 66 / 100) train acc: 0.980000; val_acc: 0.921667\n",
      "(Epoch 67 / 100) train acc: 0.985000; val_acc: 0.922500\n",
      "(Epoch 68 / 100) train acc: 0.977000; val_acc: 0.920833\n",
      "(Epoch 69 / 100) train acc: 0.986000; val_acc: 0.914167\n",
      "(Epoch 70 / 100) train acc: 0.990000; val_acc: 0.916667\n",
      "(Epoch 71 / 100) train acc: 0.979000; val_acc: 0.922500\n",
      "(Epoch 72 / 100) train acc: 0.989000; val_acc: 0.917500\n",
      "(Epoch 73 / 100) train acc: 0.987000; val_acc: 0.915833\n",
      "(Epoch 74 / 100) train acc: 0.987000; val_acc: 0.920833\n",
      "(Epoch 75 / 100) train acc: 0.980000; val_acc: 0.918333\n",
      "(Epoch 76 / 100) train acc: 0.991000; val_acc: 0.920833\n",
      "(Epoch 77 / 100) train acc: 0.986000; val_acc: 0.912500\n",
      "(Epoch 78 / 100) train acc: 0.992000; val_acc: 0.921667\n",
      "(Epoch 79 / 100) train acc: 0.988000; val_acc: 0.919167\n",
      "(Epoch 80 / 100) train acc: 0.990000; val_acc: 0.924167\n",
      "(Epoch 81 / 100) train acc: 0.979000; val_acc: 0.925833\n",
      "(Epoch 82 / 100) train acc: 0.988000; val_acc: 0.921667\n",
      "(Epoch 83 / 100) train acc: 0.992000; val_acc: 0.923333\n",
      "(Iteration 3001 / 3600) loss: 5.189778\n",
      "(Epoch 84 / 100) train acc: 0.989000; val_acc: 0.922500\n",
      "(Epoch 85 / 100) train acc: 0.984000; val_acc: 0.922500\n",
      "(Epoch 86 / 100) train acc: 0.992000; val_acc: 0.921667\n",
      "(Epoch 87 / 100) train acc: 0.989000; val_acc: 0.925000\n",
      "(Epoch 88 / 100) train acc: 0.989000; val_acc: 0.922500\n",
      "(Epoch 89 / 100) train acc: 0.994000; val_acc: 0.922500\n",
      "(Epoch 90 / 100) train acc: 0.987000; val_acc: 0.921667\n",
      "(Epoch 91 / 100) train acc: 0.993000; val_acc: 0.922500\n",
      "(Epoch 92 / 100) train acc: 0.993000; val_acc: 0.925000\n",
      "(Epoch 93 / 100) train acc: 0.990000; val_acc: 0.921667\n",
      "(Epoch 94 / 100) train acc: 0.989000; val_acc: 0.920000\n",
      "(Epoch 95 / 100) train acc: 0.993000; val_acc: 0.920833\n",
      "(Epoch 96 / 100) train acc: 0.988000; val_acc: 0.920833\n",
      "(Epoch 97 / 100) train acc: 0.987000; val_acc: 0.921667\n",
      "(Epoch 98 / 100) train acc: 0.989000; val_acc: 0.919167\n",
      "(Epoch 99 / 100) train acc: 0.991000; val_acc: 0.924167\n",
      "(Epoch 100 / 100) train acc: 0.991000; val_acc: 0.922500\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "      'X_train': feat_train,\n",
    "      'y_train': label_train,\n",
    "      'X_val': feat_val,\n",
    "      'y_val': label_val}\n",
    "\n",
    "# TODO: fill out the hyperparamets\n",
    "hyperparams = {'lr_decay': .99,\n",
    "               'num_epochs': 100,\n",
    "               'batch_size': 100,\n",
    "               'learning_rate': 1e-5,\n",
    "               'weight_scale': .01\n",
    "              }\n",
    "\n",
    "# TODO: fill out the number of units in your hidden layers\n",
    "hidden_dim = [150, 150] # this should be a list of units for each hiddent layer\n",
    "\n",
    "model = FullyConnectedNet(input_dim=75,\n",
    "                          hidden_dim=hidden_dim, \n",
    "                          weight_scale = hyperparams['weight_scale'])\n",
    "solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': hyperparams['learning_rate'],\n",
    "                },\n",
    "                lr_decay=hyperparams['lr_decay'],\n",
    "                num_epochs=hyperparams['num_epochs'], \n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 3600) loss: 2481.250907\n",
      "(Epoch 0 / 100) train acc: 0.181000; val_acc: 0.204167\n",
      "(Epoch 1 / 100) train acc: 0.475000; val_acc: 0.424167\n",
      "(Epoch 2 / 100) train acc: 0.539000; val_acc: 0.484167\n",
      "(Epoch 3 / 100) train acc: 0.636000; val_acc: 0.580000\n",
      "(Epoch 4 / 100) train acc: 0.509000; val_acc: 0.445833\n",
      "(Epoch 5 / 100) train acc: 0.579000; val_acc: 0.576667\n",
      "(Epoch 6 / 100) train acc: 0.724000; val_acc: 0.681667\n",
      "(Epoch 7 / 100) train acc: 0.748000; val_acc: 0.712500\n",
      "(Epoch 8 / 100) train acc: 0.808000; val_acc: 0.714167\n",
      "(Epoch 9 / 100) train acc: 0.767000; val_acc: 0.718333\n",
      "(Epoch 10 / 100) train acc: 0.704000; val_acc: 0.670000\n",
      "(Epoch 11 / 100) train acc: 0.630000; val_acc: 0.581667\n",
      "(Epoch 12 / 100) train acc: 0.750000; val_acc: 0.725833\n",
      "(Epoch 13 / 100) train acc: 0.832000; val_acc: 0.775833\n",
      "(Epoch 14 / 100) train acc: 0.851000; val_acc: 0.785833\n",
      "(Epoch 15 / 100) train acc: 0.861000; val_acc: 0.790000\n",
      "(Epoch 16 / 100) train acc: 0.849000; val_acc: 0.774167\n",
      "(Epoch 17 / 100) train acc: 0.859000; val_acc: 0.790000\n",
      "(Epoch 18 / 100) train acc: 0.866000; val_acc: 0.797500\n",
      "(Epoch 19 / 100) train acc: 0.888000; val_acc: 0.809167\n",
      "(Epoch 20 / 100) train acc: 0.884000; val_acc: 0.808333\n",
      "(Epoch 21 / 100) train acc: 0.880000; val_acc: 0.809167\n",
      "(Epoch 22 / 100) train acc: 0.892000; val_acc: 0.821667\n",
      "(Epoch 23 / 100) train acc: 0.900000; val_acc: 0.821667\n",
      "(Epoch 24 / 100) train acc: 0.907000; val_acc: 0.827500\n",
      "(Epoch 25 / 100) train acc: 0.891000; val_acc: 0.823333\n",
      "(Epoch 26 / 100) train acc: 0.895000; val_acc: 0.834167\n",
      "(Epoch 27 / 100) train acc: 0.913000; val_acc: 0.835833\n",
      "(Iteration 1001 / 3600) loss: 34.540175\n",
      "(Epoch 28 / 100) train acc: 0.898000; val_acc: 0.832500\n",
      "(Epoch 29 / 100) train acc: 0.913000; val_acc: 0.844167\n",
      "(Epoch 30 / 100) train acc: 0.914000; val_acc: 0.844167\n",
      "(Epoch 31 / 100) train acc: 0.911000; val_acc: 0.846667\n",
      "(Epoch 32 / 100) train acc: 0.921000; val_acc: 0.846667\n",
      "(Epoch 33 / 100) train acc: 0.927000; val_acc: 0.844167\n",
      "(Epoch 34 / 100) train acc: 0.925000; val_acc: 0.844167\n",
      "(Epoch 35 / 100) train acc: 0.931000; val_acc: 0.846667\n",
      "(Epoch 36 / 100) train acc: 0.920000; val_acc: 0.854167\n",
      "(Epoch 37 / 100) train acc: 0.946000; val_acc: 0.855833\n",
      "(Epoch 38 / 100) train acc: 0.922000; val_acc: 0.854167\n",
      "(Epoch 39 / 100) train acc: 0.953000; val_acc: 0.850833\n",
      "(Epoch 40 / 100) train acc: 0.934000; val_acc: 0.850000\n",
      "(Epoch 41 / 100) train acc: 0.937000; val_acc: 0.854167\n",
      "(Epoch 42 / 100) train acc: 0.931000; val_acc: 0.856667\n",
      "(Epoch 43 / 100) train acc: 0.917000; val_acc: 0.855000\n",
      "(Epoch 44 / 100) train acc: 0.918000; val_acc: 0.853333\n",
      "(Epoch 45 / 100) train acc: 0.937000; val_acc: 0.860000\n",
      "(Epoch 46 / 100) train acc: 0.948000; val_acc: 0.862500\n",
      "(Epoch 47 / 100) train acc: 0.929000; val_acc: 0.860833\n",
      "(Epoch 48 / 100) train acc: 0.933000; val_acc: 0.863333\n",
      "(Epoch 49 / 100) train acc: 0.942000; val_acc: 0.866667\n",
      "(Epoch 50 / 100) train acc: 0.946000; val_acc: 0.860000\n",
      "(Epoch 51 / 100) train acc: 0.934000; val_acc: 0.866667\n",
      "(Epoch 52 / 100) train acc: 0.937000; val_acc: 0.865000\n",
      "(Epoch 53 / 100) train acc: 0.946000; val_acc: 0.868333\n",
      "(Epoch 54 / 100) train acc: 0.935000; val_acc: 0.866667\n",
      "(Epoch 55 / 100) train acc: 0.948000; val_acc: 0.869167\n",
      "(Iteration 2001 / 3600) loss: 23.888662\n",
      "(Epoch 56 / 100) train acc: 0.952000; val_acc: 0.870833\n",
      "(Epoch 57 / 100) train acc: 0.937000; val_acc: 0.869167\n",
      "(Epoch 58 / 100) train acc: 0.940000; val_acc: 0.872500\n",
      "(Epoch 59 / 100) train acc: 0.961000; val_acc: 0.870833\n",
      "(Epoch 60 / 100) train acc: 0.942000; val_acc: 0.872500\n",
      "(Epoch 61 / 100) train acc: 0.939000; val_acc: 0.871667\n",
      "(Epoch 62 / 100) train acc: 0.932000; val_acc: 0.870833\n",
      "(Epoch 63 / 100) train acc: 0.953000; val_acc: 0.875833\n",
      "(Epoch 64 / 100) train acc: 0.950000; val_acc: 0.874167\n",
      "(Epoch 65 / 100) train acc: 0.950000; val_acc: 0.873333\n",
      "(Epoch 66 / 100) train acc: 0.963000; val_acc: 0.874167\n",
      "(Epoch 67 / 100) train acc: 0.949000; val_acc: 0.875833\n",
      "(Epoch 68 / 100) train acc: 0.952000; val_acc: 0.875833\n",
      "(Epoch 69 / 100) train acc: 0.953000; val_acc: 0.874167\n",
      "(Epoch 70 / 100) train acc: 0.957000; val_acc: 0.874167\n",
      "(Epoch 71 / 100) train acc: 0.949000; val_acc: 0.877500\n",
      "(Epoch 72 / 100) train acc: 0.950000; val_acc: 0.875000\n",
      "(Epoch 73 / 100) train acc: 0.946000; val_acc: 0.874167\n",
      "(Epoch 74 / 100) train acc: 0.952000; val_acc: 0.876667\n",
      "(Epoch 75 / 100) train acc: 0.952000; val_acc: 0.878333\n",
      "(Epoch 76 / 100) train acc: 0.944000; val_acc: 0.877500\n",
      "(Epoch 77 / 100) train acc: 0.952000; val_acc: 0.874167\n",
      "(Epoch 78 / 100) train acc: 0.961000; val_acc: 0.881667\n",
      "(Epoch 79 / 100) train acc: 0.959000; val_acc: 0.880000\n",
      "(Epoch 80 / 100) train acc: 0.951000; val_acc: 0.881667\n",
      "(Epoch 81 / 100) train acc: 0.948000; val_acc: 0.873333\n",
      "(Epoch 82 / 100) train acc: 0.964000; val_acc: 0.875833\n",
      "(Epoch 83 / 100) train acc: 0.953000; val_acc: 0.881667\n",
      "(Iteration 3001 / 3600) loss: 27.702066\n",
      "(Epoch 84 / 100) train acc: 0.958000; val_acc: 0.880000\n",
      "(Epoch 85 / 100) train acc: 0.964000; val_acc: 0.877500\n",
      "(Epoch 86 / 100) train acc: 0.947000; val_acc: 0.879167\n",
      "(Epoch 87 / 100) train acc: 0.967000; val_acc: 0.879167\n",
      "(Epoch 88 / 100) train acc: 0.950000; val_acc: 0.873333\n",
      "(Epoch 89 / 100) train acc: 0.954000; val_acc: 0.880833\n",
      "(Epoch 90 / 100) train acc: 0.954000; val_acc: 0.881667\n",
      "(Epoch 91 / 100) train acc: 0.955000; val_acc: 0.878333\n",
      "(Epoch 92 / 100) train acc: 0.950000; val_acc: 0.882500\n",
      "(Epoch 93 / 100) train acc: 0.943000; val_acc: 0.884167\n",
      "(Epoch 94 / 100) train acc: 0.953000; val_acc: 0.883333\n",
      "(Epoch 95 / 100) train acc: 0.958000; val_acc: 0.881667\n",
      "(Epoch 96 / 100) train acc: 0.958000; val_acc: 0.880000\n",
      "(Epoch 97 / 100) train acc: 0.956000; val_acc: 0.884167\n",
      "(Epoch 98 / 100) train acc: 0.956000; val_acc: 0.883333\n",
      "(Epoch 99 / 100) train acc: 0.955000; val_acc: 0.885000\n",
      "(Epoch 100 / 100) train acc: 0.956000; val_acc: 0.885000\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "      'X_train': feat_train,\n",
    "      'y_train': label_train,\n",
    "      'X_val': feat_val,\n",
    "      'y_val': label_val}\n",
    "\n",
    "# TODO: fill out the hyperparamets\n",
    "hyperparams = {'lr_decay': 0.99,\n",
    "               'num_epochs': 100,\n",
    "               'batch_size': 100,\n",
    "               'learning_rate': 1e-5\n",
    "              }\n",
    "\n",
    "# TODO: fill out the number of units in your hidden layers\n",
    "hidden_dim = [30, 20, 20] # this should be a list of units for each hiddent layer\n",
    "\n",
    "model = FullyConnectedNet(input_dim=75,\n",
    "                          hidden_dim=hidden_dim)\n",
    "solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': hyperparams['learning_rate'],\n",
    "                },\n",
    "                lr_decay=hyperparams['lr_decay'],\n",
    "                num_epochs=hyperparams['num_epochs'], \n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                print_every=1000)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
